{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "from typing import Annotated, Dict, List\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "AVG_OVERX_ROWS = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE_INDEX='LSH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryFile:\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.vec_size = 70\n",
    "        self.float_size = 4\n",
    "        self.int_size = 4\n",
    "\n",
    "    def insert_row(self, row_id, row_data):\n",
    "        with open(self.filename, 'ab') as file:\n",
    "            # Pack the ID and the float values into a binary format\n",
    "            packed_data = struct.pack(f'i{self.vec_size}f', row_id, *row_data)\n",
    "            # Write the packed data to the file\n",
    "            file.write(packed_data)\n",
    "\n",
    "    def read_row(self, row_id):\n",
    "        with open(self.filename, 'rb') as file:\n",
    "            # Calculate the position of the row\n",
    "            # Size of one row (ID + vec_size * floats)\n",
    "            position = row_id * \\\n",
    "                (self.int_size + self.vec_size * self.float_size)\n",
    "            # Seek to the position of the row\n",
    "            file.seek(position)\n",
    "            # Read the row\n",
    "            # Size of one row (ID + vec_size * floats)\n",
    "            packed_data = file.read(\n",
    "                self.int_size + self.vec_size * self.float_size)\n",
    "            data = struct.unpack(f'i{self.vec_size}f', packed_data)\n",
    "            return np.array(data)\n",
    "\n",
    "    def insert_records(self, rows: List[Dict[int, Annotated[List[float], 70]]]):\n",
    "        first_position = None\n",
    "        last_position = None\n",
    "        with open(self.filename, 'ab') as file:\n",
    "            # record the position before writing\n",
    "            first_position = file.tell()\n",
    "            for row in rows:\n",
    "                id, embed = row[\"id\"], row[\"embed\"]\n",
    "                # Pack the ID and the float values into a binary format\n",
    "                packed_data = struct.pack(f'i{self.vec_size}f', id, *embed)\n",
    "                # Write the packed data to the file\n",
    "                file.write(packed_data)\n",
    "            # Record the position after writing\n",
    "            last_position = file.tell()\n",
    "        # Return the first and last position\n",
    "        return first_position, last_position\n",
    "\n",
    "    # read all rows\n",
    "    def read_all(self):\n",
    "        rows = []\n",
    "        with open(self.filename, 'rb') as file:\n",
    "            # iterate over all rows\n",
    "            while True:\n",
    "                # Read the row\n",
    "                packed_data = file.read(\n",
    "                    self.int_size + self.vec_size * self.float_size)\n",
    "                if packed_data == b'':\n",
    "                    break\n",
    "                data = struct.unpack(f'i{self.vec_size}f', packed_data)\n",
    "                rows.append(data)\n",
    "        return np.array(rows)\n",
    "\n",
    "    def read_positions_in_range(self, first_position, last_position):\n",
    "        records = []\n",
    "        with open(self.filename, 'rb') as file:\n",
    "            file.seek(first_position)\n",
    "            while file.tell() < last_position:\n",
    "                packed_data = file.read(\n",
    "                    self.int_size + self.vec_size * self.float_size)\n",
    "                if packed_data == b'':\n",
    "                    break\n",
    "                data = struct.unpack(f'i{self.vec_size}f', packed_data)\n",
    "                records.append(data)\n",
    "        return np.array(records)\n",
    "\n",
    "    def insert_position(self, row_id, position):\n",
    "        with open(self.filename, 'ab') as file:\n",
    "            packed_data = struct.pack('iii', row_id, *position)\n",
    "            file.write(packed_data)\n",
    "\n",
    "    def read_position(self, row_id):\n",
    "        with open(self.filename, 'rb') as file:\n",
    "            position = row_id * (self.int_size * 2 + self.int_size)\n",
    "            file.seek(position)\n",
    "            packed_data = file.read(self.int_size * 3)\n",
    "            data = struct.unpack('iii', packed_data)\n",
    "            return np.array(data)\n",
    "\n",
    "    def insert_positions(self, rows: List[Dict[int, List[int]]]):\n",
    "        with open(self.filename, 'ab') as file:\n",
    "            for row in rows:\n",
    "                id, position = row[\"id\"], row[\"position\"]\n",
    "                packed_data = struct.pack('iii', id, *position)\n",
    "                file.write(packed_data)\n",
    "\n",
    "    def read_all_positions(self):\n",
    "        positions = []\n",
    "        with open(self.filename, 'rb') as file:\n",
    "            while True:\n",
    "                packed_data = file.read(self.int_size * 3)\n",
    "                if packed_data == b'':\n",
    "                    break\n",
    "                data = struct.unpack('iii', packed_data)\n",
    "                positions.append(data)\n",
    "        return np.array(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insertion if saved_db is not created\n",
    "\n",
    "class CreateDatabase:\n",
    "    def __init__(self, file_path=\"saved_db.bin\", new_db=True) -> None:\n",
    "        self.file_path = file_path\n",
    "        # binary file handler\n",
    "        self.bfh = BinaryFile(self.file_path)\n",
    "        if new_db:\n",
    "            # just open new file to delete the old one\n",
    "            with open(self.file_path, \"w\") as fout:\n",
    "                # if you need to add any head to the file\n",
    "                pass\n",
    "\n",
    "    def insert_records(self, rows: List[Dict[int, Annotated[List[float], 70]]]):\n",
    "        self.bfh.insert_records(rows)\n",
    "\n",
    "\n",
    "db = CreateDatabase(file_path=\"saved_db.bin\")\n",
    "records_np = np.random.random((1000000, 70))\n",
    "records_dict = [{\"id\": i, \"embed\": list(row)}\n",
    "                for i, row in enumerate(records_np)]\n",
    "db.insert_records(records_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IvfDb:\n",
    "    def __init__(self, file_path=\"saved_db.bin\") -> None:\n",
    "        self.file_path = file_path\n",
    "        # number of clusters\n",
    "        self.n_clusters1 = 1000\n",
    "        # self.n_clusters2 = 10\n",
    "        # binary file handler\n",
    "        self.bfh = BinaryFile(self.file_path)\n",
    "        ######################\n",
    "        # LSH config\n",
    "        self.dim = 70\n",
    "        self.num_hashes = 10\n",
    "\n",
    "\n",
    "    def rertive_embeddings(self):\n",
    "        # return all rows\n",
    "        return self.bfh.read_all()[:, 1:]\n",
    "\n",
    "\n",
    "    #############################################################\n",
    "    ############     search with cos similarity     #############\n",
    "    #############################################################\n",
    "    def _search_with_cos_similarity(self, position_file, cluster_file, scores_id_array, query, top_in_region_num, top_results_num):\n",
    "      bfh_c_pos = BinaryFile(position_file)\n",
    "      bfh_c = BinaryFile(cluster_file)\n",
    "      top_results = []\n",
    "      for score in scores_id_array:\n",
    "          # read position of this cluster index (centroid index)\n",
    "          first_position, second_position = bfh_c_pos.read_position(int(score[1]))[1:]\n",
    "          # read all vectors in this cluster as [[], [], [], ...]\n",
    "          region_vectors = bfh_c.read_positions_in_range(first_position, second_position)\n",
    "          region_vectors_scores = []\n",
    "          for vec in region_vectors:\n",
    "              # read id and features of this vector\n",
    "              id = vec[0]\n",
    "              embed = vec[1:]\n",
    "              vector_score = self._cal_score(query, embed)\n",
    "              region_vectors_scores.append((vector_score, id))\n",
    "\n",
    "          # get k (top_in_region_num) the nearest vectors of that region\n",
    "          region_vectors_scores = sorted(region_vectors_scores, reverse=True)[:top_in_region_num]\n",
    "          # concat to get all results of all regions\n",
    "          top_results = top_results + region_vectors_scores\n",
    "\n",
    "      # take the best k (top_results_num) vectors in those vectors\n",
    "      top_results = sorted(top_results, reverse=True)[:top_results_num]\n",
    "\n",
    "      # the top_results here has scores and ids sorted on scores\n",
    "      return top_results\n",
    "    \n",
    "\n",
    "    #############################################################\n",
    "    ##################     search with knn     ##################\n",
    "    #############################################################\n",
    "    def _search_with_knn(self, position_file, cluster_file, scores_id_array, query, top_results_num):\n",
    "      \n",
    "      all_regions_vec = []\n",
    "      classes = []\n",
    "\n",
    "      vec_id_dict = dict()\n",
    "\n",
    "      bfh_c_pos = BinaryFile(position_file)\n",
    "      bfh_c = BinaryFile(cluster_file)\n",
    "\n",
    "      # collect all vectors in all regions to train the knn on\n",
    "      for score in scores_id_array:\n",
    "          # read position of this cluster index (centroid index)\n",
    "          first_position, second_position = bfh_c_pos.read_position(int(score[1]))[1:]\n",
    "          # read all vectors in this cluster as [[], [], [], ...]\n",
    "          region_vectors = bfh_c.read_positions_in_range(first_position, second_position)\n",
    "          for vec in region_vectors:\n",
    "              # save all vectors and their ids to retrive them back after training the model\n",
    "              vec_id_dict.update({tuple(vec[1:]): vec[0]})\n",
    "              # save all embeddings to train the model on\n",
    "              all_regions_vec.append(vec[1:])\n",
    "\n",
    "          # save the classes of all vectors to train the knn on\n",
    "          classes = classes + [score[1] for _ in range(len(region_vectors))]\n",
    "\n",
    "      # train the knn model with number of neighbors = 10\n",
    "      knn = KNeighborsClassifier(n_neighbors=10)\n",
    "      knn.fit(all_regions_vec, classes)\n",
    "\n",
    "      # this have the ids of nearest vectors to the query\n",
    "      predictions = knn.kneighbors(query, return_distance=False)\n",
    "\n",
    "      top_results = []\n",
    "      for vec in predictions[0]:\n",
    "          vector_score = self._cal_score(query, all_regions_vec[vec])\n",
    "          # get the id of this vector from the dict we saved before\n",
    "          top_results.append((vector_score, vec_id_dict.get(tuple(all_regions_vec[vec]))))\n",
    "      top_results = sorted(top_results, reverse=True)[:top_results_num]\n",
    "\n",
    "      # the top_results here has scores and ids sorted on scores\n",
    "      return top_results\n",
    "\n",
    "\n",
    "    #############################################################\n",
    "    #############     our rock star retrive     #################\n",
    "    #############################################################\n",
    "    def retrive(self, query: Annotated[List[float], 70], top_k=5):\n",
    "        if TYPE_INDEX == 'LSH':\n",
    "            # Flatten the query and compute its hash\n",
    "            query = query.flatten()\n",
    "            hash_value = self.hash(query)\n",
    "\n",
    "            # read all hash key from file\n",
    "            bfh_hash_table_pos = BinaryFile('positions_hash_table.bin')\n",
    "            hash_keys = bfh_hash_table_pos.read_all_positions()\n",
    "            # bfh for the hashtable\n",
    "            bfh_hash_table = BinaryFile('hash_table.bin')\n",
    "            \n",
    "            # Generate all possible hashes\n",
    "            hashes = self.generate_hashes(hash_value)\n",
    "\n",
    "            # Initialize an empty list to store the scores\n",
    "            region_vectors_scores = []\n",
    "\n",
    "            # For each hash, retrieve the vectors and compute their scores\n",
    "            for hash_value in hashes:\n",
    "                if not(hash_value in hash_keys):\n",
    "                    continue\n",
    "\n",
    "                # getindex of hash_value in hash_keys\n",
    "                index = np.where(hash_keys[:,0] == hash_value)[0][0]\n",
    "                # Get the position of this hash in the file\n",
    "                first_position, second_position = bfh_hash_table_pos.read_position(int(index))[1:]\n",
    "\n",
    "                # Read all vectors in this hash\n",
    "                region_vectors = bfh_hash_table.read_positions_in_range(first_position, second_position)\n",
    "\n",
    "                # For each vector, compute its score and add it to the list\n",
    "                for vec in region_vectors:\n",
    "                    id = vec[0]\n",
    "                    embed = vec[1:]\n",
    "                    vector_score = self._cal_score(query, embed)\n",
    "                    region_vectors_scores.append((vector_score, id))\n",
    "\n",
    "            # Sort the scores and return the top_k IDs\n",
    "            region_vectors_scores = sorted(region_vectors_scores, reverse=True)[:top_k]\n",
    "            return [score[1] for score in region_vectors_scores]\n",
    "        else:\n",
    "            scores = []\n",
    "            centroids_level2 = BinaryFile('centroids_1.bin').read_all()\n",
    "            for centroid in centroids_level2:\n",
    "                score_centroid = self._cal_score(query, centroid[1:])\n",
    "                id = centroid[0]\n",
    "                scores.append((score_centroid, id))\n",
    "            scores = sorted(scores, reverse=True)[:30]  \n",
    "\n",
    "            top_results_level_1 = self._search_with_cos_similarity('positions_cluster_1.bin', 'cluster_1.bin', scores, query, 30, top_k)\n",
    "\n",
    "            # top_results_level_1 = self._search_with_knn('positions_cluster_1.bin', 'cluster_1.bin', scores, query, top_k)\n",
    "\n",
    "            # here we assume that if two rows have the same score, return the lowest ID\n",
    "            return [score[1] for score in top_results_level_1]\n",
    "\n",
    "\n",
    "    #############################################################\n",
    "    ####################     clc score     ######################\n",
    "    #############################################################\n",
    "    def _cal_score(self, vec1, vec2):\n",
    "        dot_product = np.dot(vec1, vec2)\n",
    "        norm_vec1 = np.linalg.norm(vec1)\n",
    "        norm_vec2 = np.linalg.norm(vec2)\n",
    "        cosine_similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "        return cosine_similarity\n",
    "\n",
    "\n",
    "\n",
    "    #############################################################\n",
    "    ###########     save clusters and positions     #############\n",
    "    #############################################################\n",
    "    def write_to_file(self, cluster_file_name, position_file_name ,centroids_file_name, clusters, centroids):\n",
    "        # save all cluster in a file\n",
    "        # output 2 file cluster, position\n",
    "        # insert clusters\n",
    "        # insert each cluster in a file\n",
    "        open(cluster_file_name, 'w').close()\n",
    "        open(position_file_name, 'w').close()\n",
    "        open(centroids_file_name, 'w').close()\n",
    "\n",
    "        bfh_c = BinaryFile(cluster_file_name)\n",
    "        bfh_c_pos = BinaryFile(position_file_name)\n",
    "        bfh_cen = BinaryFile(centroids_file_name)\n",
    "\n",
    "        # insert clusters and positions\n",
    "        for cluster_index, cluster_vectors in enumerate(clusters):\n",
    "            cluster_dict = [{\"id\": int(row[0]), \"embed\": row[1:]} for row in cluster_vectors]\n",
    "            first_position, last_position = bfh_c.insert_records(cluster_dict)\n",
    "            bfh_c_pos.insert_position(cluster_index, [first_position, last_position])\n",
    "\n",
    "        # insert centroids\n",
    "        centroids_dict = [{\"id\": i, \"embed\": row} for i, row in enumerate(centroids)]\n",
    "        bfh_cen.insert_records(centroids_dict)\n",
    "\n",
    "        for cluster_index, cluster_vectors in enumerate(clusters):\n",
    "          print(f\"Cluster {cluster_index} has {len(cluster_vectors)} vectors.\")\n",
    "\n",
    "\n",
    "    #############################################################\n",
    "    #############      partial train kmeans    ##################\n",
    "    #############################################################\n",
    "    def partial_predict(self, embeds, n_clusters):\n",
    "\n",
    "        # training_set = embeds[np.random.randint(len(embeds), size=100000 if len(embeds) > 100000 else 1000)]\n",
    "        training_set = shuffle(embeds)[:1000]\n",
    "\n",
    "        kmeans = KMeans(n_clusters=n_clusters)\n",
    "        kmeans.fit([tuple(embed) for embed in training_set])\n",
    "\n",
    "        # predict rest of data\n",
    "        cluster_labels = []\n",
    "        for i, embed in enumerate(embeds):\n",
    "            cluster_id = kmeans.predict([tuple(embed)])\n",
    "            cluster_labels.append(cluster_id)\n",
    "\n",
    "        # centroids which are list of vectors (70 float each)\n",
    "        centroids = kmeans.cluster_centers_.tolist()\n",
    "\n",
    "        return centroids, cluster_labels\n",
    "    \n",
    "\n",
    "    #############################################################\n",
    "    #################      training kmeans    ###################\n",
    "    #############################################################\n",
    "    def kmeans_training(self, embeds, n_clusters):\n",
    "\n",
    "        # kmeans = MiniBatchKMeans(\n",
    "        #     n_clusters=self.n_clusters1, batch_size=1000, random_state=42, n_init=10)\n",
    "        kmeans = KMeans(n_clusters=n_clusters)\n",
    "        kmeans.fit([tuple(embed) for embed in embeds])\n",
    "\n",
    "        # get the labels id of each cluster list of size db each vector to its cluster\n",
    "        cluster_labels = kmeans.labels_\n",
    "        centroids = kmeans.cluster_centers_.tolist()\n",
    "\n",
    "        return centroids, cluster_labels\n",
    "    \n",
    "    #############################################################\n",
    "    ###############      training lsh forest    #################\n",
    "    #############################################################\n",
    "    '''\n",
    "    lsh it generates random vectors and make hash to insert in buckets\n",
    "    '''\n",
    "    def lsh_training(self, rows):\n",
    "        self.random_vectors = np.random.randn(self.dim, self.num_hashes)\n",
    "        hash_table = {}\n",
    "        for row in rows:\n",
    "            hash_value = self.hash(row[1:])\n",
    "            if hash_value in hash_table:\n",
    "                hash_table[hash_value].append(row)\n",
    "            else:\n",
    "                hash_table[hash_value] = [row]\n",
    "\n",
    "        return hash_table\n",
    "    \n",
    "    def hash(self, input_vector):\n",
    "        \n",
    "        # Normalize the input vector and the random vectors\n",
    "        norm_input_vector = input_vector / np.linalg.norm(input_vector)\n",
    "        norm_random_vectors = self.random_vectors / np.linalg.norm(self.random_vectors, axis=0)\n",
    "\n",
    "        # Compute the cosine similarity\n",
    "        cos_sim = np.dot(norm_input_vector, norm_random_vectors)\n",
    "\n",
    "        # Determine if the cosine similarity is greater than 0\n",
    "        bools = (cos_sim > 0).astype('int')\n",
    "\n",
    "        # Convert the boolean array to a binary string and then to an integer\n",
    "        return int(''.join(bools.astype('str')), 2)\n",
    "\n",
    "    def hash2(self, input_vector):\n",
    "        bools = (np.dot(input_vector, self.random_vectors) > 0).astype('int')\n",
    "        return int(''.join(bools.astype('str')), 2)\n",
    "    \n",
    "    #  write lsh hash table in file with binaryfilehandler\n",
    "    def write_lsh_hash_table(self, hash_table, file_name, position_file_name):\n",
    "        open(file_name, 'w').close()\n",
    "        open(position_file_name, 'w').close()\n",
    "\n",
    "        bfh = BinaryFile(file_name)\n",
    "        bfh_pos = BinaryFile(position_file_name)\n",
    "\n",
    "        for hash_value, rows in hash_table.items():\n",
    "            hash_dict = [{\"id\": int(row[0]), \"embed\": row[1:]} for row in rows]\n",
    "            first_position, last_position = bfh.insert_records(hash_dict)\n",
    "            bfh_pos.insert_position(hash_value, [first_position, last_position])\n",
    "\n",
    "    def generate_hashes(self, hash_value):\n",
    "        # Convert the hash value to a binary string\n",
    "        binary_hash = format(hash_value, 'b')\n",
    "\n",
    "        # Generate all hashes by flipping one bit\n",
    "        one_bit_flips = [int(binary_hash[:i] + str(1 - int(binary_hash[i])) + binary_hash[i+1:], 2) for i in range(len(binary_hash))]\n",
    "\n",
    "        # Generate all hashes by flipping two bits\n",
    "        two_bit_flips = [int(binary_hash[:i] + str(1 - int(binary_hash[i])) + binary_hash[j+1:j] + str(1 - int(binary_hash[j])) + binary_hash[j+1:], 2) for i in range(len(binary_hash)) for j in range(i+1, len(binary_hash))]\n",
    "\n",
    "        # Return the original hash, one bit flips, and two bit flips\n",
    "        return [hash_value] + one_bit_flips + two_bit_flips\n",
    "        \n",
    "    #############################################################\n",
    "    ########     second rock star building the index     ########\n",
    "    #############################################################\n",
    "    def build_index(self):\n",
    "\n",
    "        # read all rows\n",
    "        rows = self.bfh.read_all()\n",
    "\n",
    "        if TYPE_INDEX == 'LSH':\n",
    "            ###################### level 1 ######################  \n",
    "\n",
    "            hash_table = self.lsh_training(rows)\n",
    "            self.write_lsh_hash_table(hash_table, 'hash_table.bin', 'positions_hash_table.bin')\n",
    "\n",
    "        else:\n",
    "            ###################### level 1 ######################  \n",
    "\n",
    "            # centroids, cluster_labels = self.kmeans_training(rows[:, 1:], self.n_clusters1)\n",
    "            centroids, cluster_labels = self.partial_predict(rows[:, 1:], self.n_clusters1)\n",
    "\n",
    "            clusters = [[] for _ in range(self.n_clusters1)]\n",
    "            \n",
    "            for i, label in enumerate(cluster_labels):\n",
    "                clusters[int(label)].append(rows[i])\n",
    "\n",
    "            self.write_to_file('cluster_1.bin', 'positions_cluster_1.bin', 'centroids_1.bin', clusters, centroids)\n",
    "\n",
    "\n",
    "            ###################### level 2 ######################  \n",
    "\n",
    "            # centroids_level2, cluster_labels = self.kmeans_training(centroids, self.n_clusters2)\n",
    "\n",
    "            # clusters = [[] for _ in range(self.n_clusters2)]\n",
    "\n",
    "            # for i, label in enumerate(cluster_labels):\n",
    "            #     clusters[int(label)].append([i]+ list(centroids[i]))\n",
    "\n",
    "            # self.write_to_file('cluster_2.bin', 'positions_cluster_2.bin', 'centroids_2.bin', clusters, centroids_level2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Result:\n",
    "    run_time: float\n",
    "    top_k: int\n",
    "    db_ids: List[int]\n",
    "    actual_ids: List[int]\n",
    "\n",
    "\n",
    "def run_queries(db, np_rows, top_k, num_runs):\n",
    "    results = []\n",
    "    for _ in range(num_runs):\n",
    "        query = np.random.random((1, 70))\n",
    "\n",
    "        tic = time.time()\n",
    "        db_ids = db.retrive(query, top_k)\n",
    "        toc = time.time()\n",
    "        run_time = toc - tic\n",
    "\n",
    "        tic = time.time()\n",
    "        actual_ids = np.argsort(np_rows.dot(query.T).T / (np.linalg.norm(\n",
    "            np_rows, axis=1) * np.linalg.norm(query)), axis=1).squeeze().tolist()[::-1]\n",
    "        toc = time.time()\n",
    "        np_run_time = toc - tic\n",
    "\n",
    "        results.append(Result(run_time, top_k, db_ids, actual_ids))\n",
    "    return results\n",
    "\n",
    "\n",
    "def eval(results: List[Result]):\n",
    "    # scores are negative. So getting 0 is the best score.\n",
    "    scores = []\n",
    "    run_time = []\n",
    "    for res in results:\n",
    "        run_time.append(res.run_time)\n",
    "        # case for retireving number not equal to top_k, socre will be the lowest\n",
    "        if len(set(res.db_ids)) != res.top_k or len(res.db_ids) != res.top_k:\n",
    "            print('not equal length')\n",
    "            scores.append(-1 * len(res.actual_ids) * res.top_k)\n",
    "            continue\n",
    "        score = 0\n",
    "        for id in res.db_ids:\n",
    "            try:\n",
    "                ind = res.actual_ids.index(id)\n",
    "                if ind > res.top_k * 3:\n",
    "                    print('not in first section')\n",
    "                    score -= ind\n",
    "            except:\n",
    "                print('id not exist')\n",
    "                score -= len(res.actual_ids)\n",
    "        scores.append(score)\n",
    "\n",
    "    return sum(scores) / len(scores), sum(run_time) / len(run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing and searching if saved_db created\n",
    "\n",
    "db = IvfDb(file_path=\"saved_db.bin\")\n",
    "db.build_index()\n",
    "records_np = db.rertive_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "(-65.7, 10.21569983959198)\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "(-33.2, 12.408962416648865)\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "(-2992.2, 8.245142960548401)\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "(-62.8, 7.525104427337647)\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "not in first section\n",
      "(-38.4, 11.36648063659668)\n"
     ]
    }
   ],
   "source": [
    "res = run_queries(db, records_np, 5, 10)\n",
    "print(eval(res))\n",
    "res = run_queries(db, records_np, 5, 10)\n",
    "print(eval(res))\n",
    "res = run_queries(db, records_np, 5, 10)\n",
    "print(eval(res))\n",
    "res = run_queries(db, records_np, 5, 10)\n",
    "print(eval(res))\n",
    "res = run_queries(db, records_np, 5, 10)\n",
    "print(eval(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
