{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iT_w_QuQUGT",
        "outputId": "8d3e6c09-6ab5-41f1-febb-8873ead8a592"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzA17EvbQUGF"
      },
      "outputs": [],
      "source": [
        "import struct\n",
        "from typing import Annotated, Dict, List\n",
        "import numpy as np\n",
        "import time\n",
        "from scipy.cluster.vq import vq, kmeans2\n",
        "from dataclasses import dataclass\n",
        "import random\n",
        "from sklearn.utils import shuffle\n",
        "import pickle\n",
        "\n",
        "\n",
        "\n",
        "clusters_num = 1000\n",
        "saved_file = \"saved_db_100k.bin\"\n",
        "cluster_file = \"cluster_100k.bin\"\n",
        "position_file = \"positions_100k.bin\"\n",
        "centroid_file = \"centroids_100k.bin\"\n",
        "zipped = \"100k.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1UW9QYtQUGK"
      },
      "outputs": [],
      "source": [
        "class BinaryFile:\n",
        "    def __init__(self, filename):\n",
        "        self.filename = filename\n",
        "        self.vec_size = 70\n",
        "        self.float_size = 4\n",
        "        self.int_size = 4\n",
        "\n",
        "    def insert_row(self, row_id, row_data):\n",
        "        with open(self.filename, 'ab') as file:\n",
        "            # Pack the ID and the float values into a binary format\n",
        "            packed_data = struct.pack(f'i{self.vec_size}f', row_id, *row_data)\n",
        "            # Write the packed data to the file\n",
        "            file.write(packed_data)\n",
        "\n",
        "    def read_row(self, row_id):\n",
        "        with open(self.filename, 'rb') as file:\n",
        "            # Calculate the position of the row\n",
        "            # Size of one row (ID + vec_size * floats)\n",
        "            position = row_id * \\\n",
        "                (self.int_size + self.vec_size * self.float_size)\n",
        "            # Seek to the position of the row\n",
        "            file.seek(position)\n",
        "            # Read the row\n",
        "            # Size of one row (ID + vec_size * floats)\n",
        "            packed_data = file.read(\n",
        "                self.int_size + self.vec_size * self.float_size)\n",
        "            data = struct.unpack(f'i{self.vec_size}f', packed_data)\n",
        "            return np.array(data)\n",
        "\n",
        "    def insert_records(self, rows: List[Dict[int, Annotated[List[float], 70]]]):\n",
        "        first_position = None\n",
        "        last_position = None\n",
        "        with open(self.filename, 'ab') as file:\n",
        "            # record the position before writing\n",
        "            first_position = file.tell()\n",
        "            for row in rows:\n",
        "                id, embed = row[\"id\"], row[\"embed\"]\n",
        "                # Pack the ID and the float values into a binary format\n",
        "                packed_data = struct.pack(f'i{self.vec_size}f', id, *embed)\n",
        "                # Write the packed data to the file\n",
        "                file.write(packed_data)\n",
        "            # Record the position after writing\n",
        "            last_position = file.tell()\n",
        "        # Return the first and last position\n",
        "        return first_position, last_position\n",
        "\n",
        "    # read all rows\n",
        "    def read_all(self):\n",
        "        rows = []\n",
        "        with open(self.filename, 'rb') as file:\n",
        "            # iterate over all rows\n",
        "            while True:\n",
        "                # Read the row\n",
        "                packed_data = file.read(\n",
        "                    self.int_size + self.vec_size * self.float_size)\n",
        "                if packed_data == b'':\n",
        "                    break\n",
        "                data = struct.unpack(f'i{self.vec_size}f', packed_data)\n",
        "                rows.append(data)\n",
        "        return np.array(rows)\n",
        "\n",
        "    def read_positions_in_range(self, first_position, last_position):\n",
        "        records = []\n",
        "        with open(self.filename, 'rb') as file:\n",
        "            file.seek(first_position)\n",
        "            while file.tell() < last_position:\n",
        "                packed_data = file.read(\n",
        "                    self.int_size + self.vec_size * self.float_size)\n",
        "                if packed_data == b'':\n",
        "                    break\n",
        "                data = struct.unpack(f'i{self.vec_size}f', packed_data)\n",
        "                records.append(data)\n",
        "        return np.array(records)\n",
        "\n",
        "    def insert_position(self, row_id, position):\n",
        "        with open(self.filename, 'ab') as file:\n",
        "            packed_data = struct.pack('iii', row_id, *position)\n",
        "            file.write(packed_data)\n",
        "\n",
        "    def read_position(self, row_id):\n",
        "        with open(self.filename, 'rb') as file:\n",
        "            position = row_id * (self.int_size * 2 + self.int_size)\n",
        "            file.seek(position)\n",
        "            packed_data = file.read(self.int_size * 3)\n",
        "            data = struct.unpack('iii', packed_data)\n",
        "            return np.array(data)\n",
        "\n",
        "    def insert_positions(self, rows: List[Dict[int, List[int]]]):\n",
        "        with open(self.filename, 'ab') as file:\n",
        "            for row in rows:\n",
        "                id, position = row[\"id\"], row[\"position\"]\n",
        "                packed_data = struct.pack('iii', id, *position)\n",
        "                file.write(packed_data)\n",
        "\n",
        "    def read_all_positions(self):\n",
        "        positions = []\n",
        "        with open(self.filename, 'rb') as file:\n",
        "            while True:\n",
        "                packed_data = file.read(self.int_size * 3)\n",
        "                if packed_data == b'':\n",
        "                    break\n",
        "                data = struct.unpack('iii', packed_data)\n",
        "                positions.append(data)\n",
        "        return np.array(positions)\n",
        "\n",
        "####################################################\n",
        "    # cluster_id id\n",
        "    def insert_cluster(self, row_id, row_data):\n",
        "      with open(self.filename, 'ab') as file:\n",
        "            packed_data = struct.pack('ii', row_id, row_data)\n",
        "            file.write(packed_data)\n",
        "\n",
        "    def insert_clusters(self, row_id, rows):\n",
        "        first_position = None\n",
        "        last_position = None\n",
        "        with open(self.filename, 'ab') as file:\n",
        "            # record the position before writing\n",
        "            first_position = file.tell()\n",
        "            for row in rows:\n",
        "                # Pack the ID and the float values into a binary format\n",
        "                packed_data = struct.pack('ii', row_id, row)\n",
        "                # Write the packed data to the file\n",
        "                file.write(packed_data)\n",
        "            # Record the position after writing\n",
        "            last_position = file.tell()\n",
        "        # Return the first and last position\n",
        "        return first_position, last_position\n",
        "\n",
        "    def read_clusters_in_range(self, first_position, last_position):\n",
        "        records = []\n",
        "        with open(self.filename, 'rb') as file:\n",
        "            file.seek(first_position)\n",
        "            while file.tell() < last_position:\n",
        "                packed_data = file.read(\n",
        "                    self.int_size + self.int_size)\n",
        "                if packed_data == b'':\n",
        "                    break\n",
        "                data = struct.unpack('ii', packed_data)\n",
        "                records.append(data)\n",
        "        return np.array(records)\n",
        "\n",
        "    # read all rows\n",
        "    def read_all_clusters(self):\n",
        "        rows = []\n",
        "        with open(self.filename, 'rb') as file:\n",
        "            # iterate over all rows\n",
        "            while True:\n",
        "                # Read the row\n",
        "                packed_data = file.read(\n",
        "                    self.int_size + self.int_size)\n",
        "                if packed_data == b'':\n",
        "                    break\n",
        "                data = struct.unpack('ii', packed_data)\n",
        "                rows.append(data)\n",
        "        return np.array(rows)\n",
        "\n",
        "\n",
        "####################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHRuEmiHQUGN"
      },
      "outputs": [],
      "source": [
        "# insertion if saved_db is not created\n",
        "\n",
        "class CreateDatabase:\n",
        "    def __init__(self, file_path=\"saved_db.bin\", new_db=True) -> None:\n",
        "        self.file_path = file_path\n",
        "        # binary file handler\n",
        "        self.bfh = BinaryFile(self.file_path)\n",
        "        if new_db:\n",
        "            # just open new file to delete the old one\n",
        "            with open(self.file_path, \"w\") as fout:\n",
        "                # if you need to add any head to the file\n",
        "                pass\n",
        "\n",
        "    def insert_records(self, rows: List[Dict[int, Annotated[List[float], 70]]]):\n",
        "        self.bfh.insert_records(rows)\n",
        "\n",
        "\n",
        "db = CreateDatabase(file_path=saved_file)\n",
        "rng = np.random.default_rng(50)\n",
        "records_np = rng.random((100000, 70), dtype=np.float32)\n",
        "records_dict = [{\"id\": j, \"embed\": list(row)}\n",
        "              for j, row in enumerate(records_np)]\n",
        "db.insert_records(records_dict)\n",
        "del records_np\n",
        "del records_dict\n",
        "del rng\n",
        "del db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsaRrpIVQUGO"
      },
      "outputs": [],
      "source": [
        "class IvfDb:\n",
        "    def __init__(self, file_path=\"saved_db.bin\", clusters_path='', positions_path='', centroids_path='') -> None:\n",
        "        self.file_path = file_path\n",
        "        # number of clusters\n",
        "        self.n_clusters1 = clusters_num\n",
        "        # self.n_clusters2 = 10\n",
        "        # binary file handler\n",
        "        self.clusters_path = clusters_path\n",
        "        self.positions_path = positions_path\n",
        "        self.centroids_path = centroids_path\n",
        "        self.bfh = BinaryFile(self.file_path)\n",
        "\n",
        "\n",
        "    def rertive_embeddings(self):\n",
        "        # return all rows\n",
        "        return self.bfh.read_all()[:, 1:]\n",
        "\n",
        "\n",
        "    #############################################################\n",
        "    ############     search with cos similarity     #############\n",
        "    #############################################################\n",
        "    def _search_with_cos_similarity(self, position_file, cluster_file, scores_id_array, query, top_in_region_num, top_results_num):\n",
        "      bfh_c_pos = BinaryFile(position_file)\n",
        "      bfh_c = BinaryFile(cluster_file)\n",
        "      top_results = []\n",
        "      for score in scores_id_array:\n",
        "          # read position of this cluster index (centroid index)\n",
        "          first_position, second_position = bfh_c_pos.read_position(int(score[1]))[1:]\n",
        "          # read all vectors in this cluster as [[], [], [], ...]\n",
        "          vec_ids = bfh_c.read_clusters_in_range(first_position, second_position)\n",
        "          region_vectors_scores = []\n",
        "          for vec_id in vec_ids:\n",
        "              vec = self.bfh.read_row(int(vec_id[1]))\n",
        "              # read id and features of this vector\n",
        "              id = vec[0]\n",
        "              embed = vec[1:]\n",
        "              vector_score = self._cal_score(query, embed)\n",
        "              region_vectors_scores.append((vector_score, id))\n",
        "\n",
        "          # get k (top_in_region_num) the nearest vectors of that region\n",
        "          region_vectors_scores = sorted(region_vectors_scores, reverse=True)[:top_in_region_num]\n",
        "          # concat to get all results of all regions\n",
        "          top_results = top_results + region_vectors_scores\n",
        "\n",
        "      # take the best k (top_results_num) vectors in those vectors\n",
        "      top_results = sorted(top_results, reverse=True)[:top_results_num]\n",
        "\n",
        "      # the top_results here has scores and ids sorted on scores\n",
        "      return top_results\n",
        "\n",
        "    #############################################################\n",
        "    #############     our rock star retrive     #################\n",
        "    #############################################################\n",
        "    def retrive(self, query: Annotated[List[float], 70], top_k=5):\n",
        "        scores = []\n",
        "        centroids_level2 = BinaryFile(self.centroids_path).read_all()\n",
        "        for centroid in centroids_level2:\n",
        "            score_centroid = self._cal_score(query, centroid[1:])\n",
        "            id = centroid[0]\n",
        "            scores.append((score_centroid, id))\n",
        "        scores = sorted(scores, reverse=True)[:30]\n",
        "\n",
        "        top_results_level_1 = self._search_with_cos_similarity(self.positions_path, self.clusters_path, scores, query, 30, top_k)\n",
        "\n",
        "        # top_results_level_1 = self._search_with_knn('positions_cluster_1.bin', 'cluster_1.bin', scores, query, top_k)\n",
        "\n",
        "        # here we assume that if two rows have the same score, return the lowest ID\n",
        "        return [score[1] for score in top_results_level_1]\n",
        "\n",
        "\n",
        "    #############################################################\n",
        "    ####################     clc score     ######################\n",
        "    #############################################################\n",
        "    def _cal_score(self, vec1, vec2):\n",
        "        dot_product = np.dot(vec1, vec2)\n",
        "        norm_vec1 = np.linalg.norm(vec1)\n",
        "        norm_vec2 = np.linalg.norm(vec2)\n",
        "        cosine_similarity = dot_product / (norm_vec1 * norm_vec2)\n",
        "        return cosine_similarity\n",
        "\n",
        "\n",
        "\n",
        "    #############################################################\n",
        "    ###########     save clusters and positions     #############\n",
        "    #############################################################\n",
        "    def write_to_file(self, cluster_file_name, position_file_name ,centroids_file_name, clusters, centroids):\n",
        "        # save all cluster in a file\n",
        "        # output 2 file cluster, position\n",
        "        # insert clusters\n",
        "        # insert each cluster in a file\n",
        "        open(cluster_file_name, 'w').close()\n",
        "        open(position_file_name, 'w').close()\n",
        "        open(centroids_file_name, 'w').close()\n",
        "\n",
        "        bfh_c = BinaryFile(cluster_file_name)\n",
        "        bfh_c_pos = BinaryFile(position_file_name)\n",
        "        bfh_cen = BinaryFile(centroids_file_name)\n",
        "\n",
        "        for cluster_index, cluster_vectors in enumerate(clusters):\n",
        "            first_position, last_position = bfh_c.insert_clusters(cluster_index, cluster_vectors)\n",
        "            bfh_c_pos.insert_position(cluster_index, [first_position, last_position])\n",
        "        #############################################################\n",
        "\n",
        "        # insert centroids\n",
        "        centroids_dict = [{\"id\": i, \"embed\": row} for i, row in enumerate(centroids)]\n",
        "        bfh_cen.insert_records(centroids_dict)\n",
        "\n",
        "        for cluster_index, cluster_vectors in enumerate(clusters):\n",
        "          print(f\"Cluster {cluster_index} has {len(cluster_vectors)} vectors.\")\n",
        "\n",
        "\n",
        "    #############################################################\n",
        "    ########     second rock star building the index     ########\n",
        "    #############################################################\n",
        "    def build_index(self):\n",
        "\n",
        "\n",
        "        # read 100 000\n",
        "        # num_ids = training_set\n",
        "        # leno = MIL * 1000000\n",
        "        ranges = 100000\n",
        "        # read all rows\n",
        "        rows = []\n",
        "        for id in ranges:\n",
        "          rows.append(self.bfh.read_row(id))\n",
        "\n",
        "        rows = np.array(rows)\n",
        "        print(len(rows))\n",
        "\n",
        "        ###################### level 1 ######################\n",
        "\n",
        "        # centroids, cluster_labels = self.kmeans_training(rows[:, 1:], self.n_clusters1)\n",
        "        # centroids, cluster_labels = self.partial_predict(rows, self.n_clusters1)\n",
        "        embeds_nour = [tuple(embed) for embed in rows[:, 1:]]\n",
        "        centroids, clusters_labels = kmeans2(embeds_nour, self.n_clusters1)\n",
        "        centroids = centroids.tolist()\n",
        "        # kmeans = KMeans(n_clusters=self.n_clusters1)\n",
        "        # kmeans.fit([tuple(embed) for embed in rows[:, 1:]])\n",
        "        ###############################################################################\n",
        "        clusters = [[] for _ in range(self.n_clusters1)]\n",
        "        # for id in range(leno):\n",
        "        #   row = self.bfh.read_row(id)\n",
        "        #   # predict\n",
        "        #   cluster_id = kmeans.predict([tuple(row[1:])])\n",
        "        #   clusters[int(cluster_id)].append(id)\n",
        "\n",
        "        veccs = [self.bfh.read_row(id) for id in range(100000)]\n",
        "        # predictions = kmeans.predict([tuple(row[1:]) for row in veccs])\n",
        "        predictions = kmeans2([tuple(row[1:]) for row in veccs], centroids, minit='matrix')[1]\n",
        "        for id, cluster_id in enumerate(predictions):\n",
        "            clusters[int(cluster_id)].append(int(veccs[id][0]))\n",
        "\n",
        "        # centroids which are list of vectors (70 float each)\n",
        "        # centroids = kmeans.cluster_centers_.tolist()\n",
        "\n",
        "\n",
        "\n",
        "        self.write_to_file(self.clusters_path, self.positions_path, self.centroids_path, clusters, centroids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1acSuDu_-2T4"
      },
      "outputs": [],
      "source": [
        "db = IvfDb(file_path=saved_file, clusters_path=cluster_file, centroids_path=centroid_file, positions_path=position_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-I9x1YwnQUGR"
      },
      "outputs": [],
      "source": [
        "# indexing and searching if saved_db created\n",
        "db.build_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChRPYWAHYPUi"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "file_paths = [f'/content/{saved_file}', f'/content/{position_file}', f'/content/{centroid_file}', f'/content/{cluster_file}']\n",
        "\n",
        "zip_file_path = f'/content/drive/MyDrive/SemanticSearchEngine/{zipped}'\n",
        "\n",
        "# Create a zip file and add the specified files to it\n",
        "with zipfile.ZipFile(zip_file_path, 'w') as zip_file:\n",
        "    for file_path in file_paths:\n",
        "        zip_file.write(file_path, arcname=file_path.split('/')[-1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/tasleem/100k.zip /content/drive/MyDrive/sematic-search-engine/v5/\n",
        "!cp /content/drive/MyDrive/tasleem/100k.zip /content/drive/MyDrive/sematic-search-engine/v6/\n",
        "\n",
        "!cp /content/drive/MyDrive/tasleem/1msci_v3.zip /content/drive/MyDrive/sematic-search-engine/v5/\n",
        "!cp /content/drive/MyDrive/tasleem/1msci_v3.zip /content/drive/MyDrive/sematic-search-engine/v6/\n",
        "\n",
        "!cp /content/drive/MyDrive/SemanticSearchEngine/5msci_hany.zip /content/drive/MyDrive/sematic-search-engine/v5/\n",
        "!cp /content/drive/MyDrive/SemanticSearchEngine/5msci_hany.zip /content/drive/MyDrive/sematic-search-engine/v6/\n",
        "\n",
        "!cp /content/drive/MyDrive/tasleem/10msci2.zip /content/drive/MyDrive/sematic-search-engine/v5/\n",
        "!cp /content/drive/MyDrive/tasleem/10msci2.zip /content/drive/MyDrive/sematic-search-engine/v6/\n",
        "\n",
        "!cp /content/drive/MyDrive/tasleem/15msci_hany.zip /content/drive/MyDrive/sematic-search-engine/v5/\n",
        "!cp /content/drive/MyDrive/tasleem/15msci_hany.zip /content/drive/MyDrive/sematic-search-engine/v6/\n",
        "\n",
        "!cp /content/drive/MyDrive/Resume/20m_final_final.zip /content/drive/MyDrive/sematic-search-engine/v5/\n",
        "!cp /content/drive/MyDrive/Resume/20m_final_final.zip /content/drive/MyDrive/sematic-search-engine/v6/"
      ],
      "metadata": {
        "id": "ryD4sLRRRiZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/sematic-search-engine/v5/100k.zip /content/drive/MyDrive/sematic-search-engine/v7/100k.zip\n",
        "!cp /content/drive/MyDrive/sematic-search-engine/v5/1msci_v3.zip /content/drive/MyDrive/sematic-search-engine/v7/1m.zip\n",
        "!cp /content/drive/MyDrive/sematic-search-engine/v5/5msci_hany.zip /content/drive/MyDrive/sematic-search-engine/v7/5m.zip\n",
        "!cp /content/drive/MyDrive/sematic-search-engine/v5/10msci2.zip /content/drive/MyDrive/sematic-search-engine/v7/10m.zip\n",
        "!cp /content/drive/MyDrive/sematic-search-engine/v5/15msci_hany.zip /content/drive/MyDrive/sematic-search-engine/v7/15m.zip\n",
        "!cp /content/drive/MyDrive/sematic-search-engine/v5/20m_final_final.zip /content/drive/MyDrive/sematic-search-engine/v7/20m.zip"
      ],
      "metadata": {
        "id": "jCcbxxB0tuQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N2LUjK-TzD2c"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}