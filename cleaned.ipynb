{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "from typing import Annotated, Dict, List\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.neighbors import LSHForest\n",
    "from dataclasses import dataclass\n",
    "\n",
    "AVG_OVERX_ROWS = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryFile:\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.vec_size = 70\n",
    "        self.float_size = 4\n",
    "        self.int_size = 4\n",
    "\n",
    "    def insert_row(self, row_id, row_data):\n",
    "        with open(self.filename, 'ab') as file:\n",
    "            # Pack the ID and the float values into a binary format\n",
    "            packed_data = struct.pack(f'i{self.vec_size}f', row_id, *row_data)\n",
    "            # Write the packed data to the file\n",
    "            file.write(packed_data)\n",
    "\n",
    "    def read_row(self, row_id):\n",
    "        with open(self.filename, 'rb') as file:\n",
    "            # Calculate the position of the row\n",
    "            # Size of one row (ID + vec_size * floats)\n",
    "            position = row_id * \\\n",
    "                (self.int_size + self.vec_size * self.float_size)\n",
    "            # Seek to the position of the row\n",
    "            file.seek(position)\n",
    "            # Read the row\n",
    "            # Size of one row (ID + vec_size * floats)\n",
    "            packed_data = file.read(\n",
    "                self.int_size + self.vec_size * self.float_size)\n",
    "            data = struct.unpack(f'i{self.vec_size}f', packed_data)\n",
    "            return np.array(data)\n",
    "\n",
    "    def insert_records(self, rows: List[Dict[int, Annotated[List[float], 70]]]):\n",
    "        first_position = None\n",
    "        last_position = None\n",
    "        with open(self.filename, 'ab') as file:\n",
    "            # record the position before writing\n",
    "            first_position = file.tell()\n",
    "            for row in rows:\n",
    "                id, embed = row[\"id\"], row[\"embed\"]\n",
    "                # Pack the ID and the float values into a binary format\n",
    "                packed_data = struct.pack(f'i{self.vec_size}f', id, *embed)\n",
    "                # Write the packed data to the file\n",
    "                file.write(packed_data)\n",
    "            # Record the position after writing\n",
    "            last_position = file.tell()\n",
    "        # Return the first and last position\n",
    "        return first_position, last_position\n",
    "\n",
    "    # read all rows\n",
    "    def read_all(self):\n",
    "        rows = []\n",
    "        with open(self.filename, 'rb') as file:\n",
    "            # iterate over all rows\n",
    "            while True:\n",
    "                # Read the row\n",
    "                packed_data = file.read(\n",
    "                    self.int_size + self.vec_size * self.float_size)\n",
    "                if packed_data == b'':\n",
    "                    break\n",
    "                data = struct.unpack(f'i{self.vec_size}f', packed_data)\n",
    "                rows.append(data)\n",
    "        return np.array(rows)\n",
    "\n",
    "    def read_positions_in_range(self, first_position, last_position):\n",
    "        records = []\n",
    "        with open(self.filename, 'rb') as file:\n",
    "            file.seek(first_position)\n",
    "            while file.tell() < last_position:\n",
    "                packed_data = file.read(\n",
    "                    self.int_size + self.vec_size * self.float_size)\n",
    "                if packed_data == b'':\n",
    "                    break\n",
    "                data = struct.unpack(f'i{self.vec_size}f', packed_data)\n",
    "                records.append(data)\n",
    "        return np.array(records)\n",
    "\n",
    "    def insert_position(self, row_id, position):\n",
    "        with open(self.filename, 'ab') as file:\n",
    "            packed_data = struct.pack('iii', row_id, *position)\n",
    "            file.write(packed_data)\n",
    "\n",
    "    def read_position(self, row_id):\n",
    "        with open(self.filename, 'rb') as file:\n",
    "            position = row_id * (self.int_size * 2 + self.int_size)\n",
    "            file.seek(position)\n",
    "            packed_data = file.read(self.int_size * 3)\n",
    "            data = struct.unpack('iii', packed_data)\n",
    "            return np.array(data)\n",
    "\n",
    "    def insert_positions(self, rows: List[Dict[int, List[int]]]):\n",
    "        with open(self.filename, 'ab') as file:\n",
    "            for row in rows:\n",
    "                id, position = row[\"id\"], row[\"position\"]\n",
    "                packed_data = struct.pack('iii', id, *position)\n",
    "                file.write(packed_data)\n",
    "\n",
    "    def read_all_positions(self):\n",
    "        positions = []\n",
    "        with open(self.filename, 'rb') as file:\n",
    "            while True:\n",
    "                packed_data = file.read(self.int_size * 3)\n",
    "                if packed_data == b'':\n",
    "                    break\n",
    "                data = struct.unpack('iii', packed_data)\n",
    "                positions.append(data)\n",
    "        return np.array(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insertion if saved_db is not created\n",
    "\n",
    "class CreateDatabase:\n",
    "    def __init__(self, file_path=\"saved_db.bin\", new_db=True) -> None:\n",
    "        self.file_path = file_path\n",
    "        # binary file handler\n",
    "        self.bfh = BinaryFile(self.file_path)\n",
    "        if new_db:\n",
    "            # just open new file to delete the old one\n",
    "            with open(self.file_path, \"w\") as fout:\n",
    "                # if you need to add any head to the file\n",
    "                pass\n",
    "\n",
    "    def insert_records(self, rows: List[Dict[int, Annotated[List[float], 70]]]):\n",
    "        # insert all rows with bfh\n",
    "        self.bfh.insert_records(rows)\n",
    "\n",
    "\n",
    "db = CreateDatabase(file_path=\"saved_db.bin\")\n",
    "records_np = np.random.random((1000, 70))\n",
    "records_dict = [{\"id\": i, \"embed\": list(row)}\n",
    "                for i, row in enumerate(records_np)]\n",
    "db.insert_records(records_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IvfDb:\n",
    "    def __init__(self, file_path=\"saved_db.bin\") -> None:\n",
    "        self.file_path = file_path\n",
    "        # number of clusters\n",
    "        self.n_clusters1 = 100\n",
    "        self.n_clusters2 = 10\n",
    "        # binary file handler\n",
    "        self.bfh = BinaryFile(self.file_path)\n",
    "\n",
    "    def rertive_embeddings(self):\n",
    "        # return all rows\n",
    "        return self.bfh.read_all()[:, 1:]\n",
    "\n",
    "\n",
    "    #############################################################\n",
    "    ############     search with cos similarity     #############\n",
    "    #############################################################\n",
    "    def _search_with_cos_similarity(self, position_file, cluster_file, scores_id_array, query, top_in_region_num, top_results_num):\n",
    "      bfh_c_pos = BinaryFile(position_file)\n",
    "      bfh_c = BinaryFile(cluster_file)\n",
    "      top_results = []\n",
    "      for score in scores_id_array:\n",
    "          # read position of this cluster index (centroid index)\n",
    "          first_position, second_position = bfh_c_pos.read_position(int(score[1]))[1:]\n",
    "          # read all vectors in this cluster as [[], [], [], ...]\n",
    "          region_vectors = bfh_c.read_positions_in_range(first_position, second_position)\n",
    "          region_vectors_scores = []\n",
    "          for vec in region_vectors:\n",
    "              # read id and features of this vector\n",
    "              id = vec[0]\n",
    "              embed = vec[1:]\n",
    "              vector_score = self._cal_score(query, embed)\n",
    "              region_vectors_scores.append((vector_score, id))\n",
    "\n",
    "          # get k (top_in_region_num) the nearest vectors of that region\n",
    "          region_vectors_scores = sorted(region_vectors_scores, reverse=True)[:top_in_region_num]\n",
    "          # concat to get all results of all regions\n",
    "          top_results = top_results + region_vectors_scores\n",
    "\n",
    "      # take the best k (top_results_num) vectors in those vectors\n",
    "      top_results = sorted(top_results, reverse=True)[:top_results_num]\n",
    "\n",
    "      # the top_results here has scores and ids sorted on scores\n",
    "      return top_results\n",
    "    \n",
    "\n",
    "    #############################################################\n",
    "    ##################     search with knn     ##################\n",
    "    #############################################################\n",
    "    def _search_with_knn(self, position_file, cluster_file, scores_id_array, query, top_results_num):\n",
    "      \n",
    "      all_regions_vec = []\n",
    "      classes = []\n",
    "\n",
    "      vec_id_dict = dict()\n",
    "\n",
    "      bfh_c_pos = BinaryFile(position_file)\n",
    "      bfh_c = BinaryFile(cluster_file)\n",
    "\n",
    "      # collect all vectors in all regions to train the knn on\n",
    "      for score in scores_id_array:\n",
    "          # read position of this cluster index (centroid index)\n",
    "          first_position, second_position = bfh_c_pos.read_position(int(score[1]))[1:]\n",
    "          # read all vectors in this cluster as [[], [], [], ...]\n",
    "          region_vectors = bfh_c.read_positions_in_range(first_position, second_position)\n",
    "          for vec in region_vectors:\n",
    "              # save all vectors and their ids to retrive them back after training the model\n",
    "              vec_id_dict.update({tuple(vec[1:]): vec[0]})\n",
    "              # save all embeddings to train the model on\n",
    "              all_regions_vec.append(vec[1:])\n",
    "\n",
    "          # save the classes of all vectors to train the knn on\n",
    "          classes = classes + [score[1] for _ in range(len(region_vectors))]\n",
    "\n",
    "      # train the knn model with number of neighbors = 10\n",
    "      knn = KNeighborsClassifier(n_neighbors=10)\n",
    "      knn.fit(all_regions_vec, classes)\n",
    "\n",
    "      # this have the ids of nearest vectors to the query\n",
    "      predictions = knn.kneighbors(query, return_distance=False)\n",
    "\n",
    "      top_results = []\n",
    "      for vec in predictions[0]:\n",
    "          vector_score = self._cal_score(query, all_regions_vec[vec])\n",
    "          # get the id of this vector from the dict we saved before\n",
    "          top_results.append((vector_score, vec_id_dict.get(tuple(all_regions_vec[vec]))))\n",
    "      top_results = sorted(top_results, reverse=True)[:top_results_num]\n",
    "\n",
    "      # the top_results here has scores and ids sorted on scores\n",
    "      return top_results\n",
    "\n",
    "\n",
    "    #############################################################\n",
    "    #############     our rock star retrive     #################\n",
    "    #############################################################\n",
    "    def retrive(self, query: Annotated[List[float], 70], top_k=5):\n",
    "\n",
    "\n",
    "        candidates = set()\n",
    "        for random_vectors, hash_table in self.tables:\n",
    "            hash_value = self.hash(query, random_vectors)\n",
    "            if hash_value in hash_table:\n",
    "                candidates.update(hash_table[hash_value])\n",
    "        neighbors = sorted([(np.linalg.norm(query - candidate), candidate) for candidate in candidates])[:num_neighbors]\n",
    "        print(neighbors)\n",
    "        return [vector for distance, vector in neighbors]\n",
    "        # scores = []\n",
    "        # h = self.hash(query)\n",
    "        # while h not in self.hash_table:\n",
    "        #     h += 1  # check the next higher hash value\n",
    "        #     if h in self.hash_table:\n",
    "        #         break\n",
    "        #     h -= 2  # check the next lower hash value\n",
    "        #     if h in self.hash_table:\n",
    "        #         break\n",
    "        # for centroid in self.hash_table[h]:\n",
    "        #     score_centroid = self._cal_score(query, centroid[1:])\n",
    "        #     id = centroid[0]\n",
    "        #     scores.append((id, score_centroid))\n",
    "        # scores = sorted(scores, reverse=True)[:5]  \n",
    "        # return [score[1] for score in scores]\n",
    "        ###################### level 1 ######################\n",
    "        scores = []\n",
    "        centroids_level2 = BinaryFile('centroids_2.bin').read_all()\n",
    "        for centroid in centroids_level2:\n",
    "            score_centroid = self._cal_score(query, centroid[1:])\n",
    "            id = centroid[0]\n",
    "            scores.append((score_centroid, id))\n",
    "        scores = sorted(scores, reverse=True)[:5]  \n",
    "\n",
    "        top_results_level_1 = self._search_with_cos_similarity('positions_cluster_2.bin', 'cluster_2.bin', scores, query, 10, 100)\n",
    "\n",
    "        top_results_level_1 = self._search_with_knn('positions_cluster_1.bin', 'cluster_1.bin', top_results_level_1, query, top_k)\n",
    "\n",
    "        # here we assume that if two rows have the same score, return the lowest ID\n",
    "        return [score[1] for score in top_results_level_1]\n",
    "\n",
    "\n",
    "    #############################################################\n",
    "    ####################     clc score     ######################\n",
    "    #############################################################\n",
    "    def _cal_score(self, vec1, vec2):\n",
    "        dot_product = np.dot(vec1, vec2)\n",
    "        norm_vec1 = np.linalg.norm(vec1)\n",
    "        norm_vec2 = np.linalg.norm(vec2)\n",
    "        cosine_similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "        return cosine_similarity\n",
    "\n",
    "\n",
    "\n",
    "    #############################################################\n",
    "    ###########     save clusters and positions     #############\n",
    "    #############################################################\n",
    "    def write_to_file(self, cluster_file_name, position_file_name ,centroids_file_name, clusters, centroids):\n",
    "        # save all cluster in a file\n",
    "        # output 2 file cluster, position\n",
    "        # insert clusters\n",
    "        # insert each cluster in a file\n",
    "        open(cluster_file_name, 'w').close()\n",
    "        open(position_file_name, 'w').close()\n",
    "        open(centroids_file_name, 'w').close()\n",
    "\n",
    "        bfh_c = BinaryFile(cluster_file_name)\n",
    "        bfh_c_pos = BinaryFile(position_file_name)\n",
    "        bfh_cen = BinaryFile(centroids_file_name)\n",
    "\n",
    "        # insert clusters and positions\n",
    "        for cluster_index, cluster_vectors in enumerate(clusters):\n",
    "            cluster_dict = [{\"id\": int(row[0]), \"embed\": row[1:]} for row in cluster_vectors]\n",
    "            first_position, last_position = bfh_c.insert_records(cluster_dict)\n",
    "            bfh_c_pos.insert_position(cluster_index, [first_position, last_position])\n",
    "\n",
    "        # insert centroids\n",
    "        centroids_dict = [{\"id\": i, \"embed\": row} for i, row in enumerate(centroids)]\n",
    "        bfh_cen.insert_records(centroids_dict)\n",
    "\n",
    "        for cluster_index, cluster_vectors in enumerate(clusters):\n",
    "          print(f\"Cluster {cluster_index} has {len(cluster_vectors)} vectors.\")\n",
    "\n",
    "\n",
    "    #############################################################\n",
    "    #############      partial train kmeans    ##################\n",
    "    #############################################################\n",
    "    def partial_predict(self, embeds, n_clusters):\n",
    "\n",
    "        training_set = embeds[np.random.randint(len(embeds), size=100000 if len(embeds) > 100000 else 1000)][:, 1:]\n",
    "\n",
    "        kmeans = KMeans(n_clusters=n_clusters)\n",
    "        kmeans.fit([tuple(embed) for embed in training_set])\n",
    "\n",
    "        # predict rest of data\n",
    "        cluster_labels = []\n",
    "        for i, embed in enumerate(embeds):\n",
    "            cluster_id = kmeans.predict([tuple(embed[1:])])\n",
    "            cluster_labels.append(cluster_id)\n",
    "\n",
    "        # centroids which are list of vectors (70 float each)\n",
    "        centroids = kmeans.cluster_centers_.tolist()\n",
    "\n",
    "        return centroids, cluster_labels\n",
    "    \n",
    "\n",
    "    #############################################################\n",
    "    #################      training kmeans    ###################\n",
    "    #############################################################\n",
    "    def kmeans_training(self, embeds, n_clusters):\n",
    "\n",
    "        # kmeans = MiniBatchKMeans(\n",
    "        #     n_clusters=self.n_clusters1, batch_size=1000, random_state=42, n_init=10)\n",
    "        kmeans = KMeans(n_clusters=n_clusters)\n",
    "        kmeans.fit([tuple(embed) for embed in embeds])\n",
    "\n",
    "        # get the labels id of each cluster list of size db each vector to its cluster\n",
    "        cluster_labels = kmeans.labels_\n",
    "        centroids = kmeans.cluster_centers_.tolist()\n",
    "\n",
    "        return centroids, cluster_labels\n",
    "        \n",
    "\n",
    "    #############################################################\n",
    "    ###############      training lsh forest    #################\n",
    "    #############################################################\n",
    "    def lsh_tree_training(self, embeds):\n",
    "\n",
    "        np.random.seed(42)  # for reproducibility\n",
    "\n",
    "        # Number of hash tables (t) and number of hash functions per table (k)\n",
    "        n_tables = 10\n",
    "        n_functions = 5\n",
    "\n",
    "        # Create LSH Forest\n",
    "        lsh_forest = LSHForest(n_estimators=n_tables, n_candidates=n_functions, random_state=42)\n",
    "\n",
    "        # Fit the LSH Forest to your data\n",
    "        lsh_forest.fit(embeds)\n",
    "\n",
    "        buckets_assigned = lsh_forest.predict(embeds)\n",
    "\n",
    "        # Get the buckets\n",
    "        buckets = lsh_forest._hash_index\n",
    "\n",
    "        return buckets, buckets_assigned\n",
    "    \n",
    "    def lsh_training(self, rows):\n",
    "        self.num_tables = 3\n",
    "        self.tables = []\n",
    "        self.dim = 70\n",
    "        self.num_hashes = [10 * i for i in range(1, self.num_tables + 1)]\n",
    "        self.num_buckets = [2 ** h for h in self.num_hashes]\n",
    "        for num_hashes, num_buckets in zip(self.num_hashes, self.num_buckets):\n",
    "            random_vectors = np.random.randn(self.dim, num_hashes)\n",
    "            hash_table = {i: [] for i in range(num_buckets)}\n",
    "            for row in rows:\n",
    "                hash_value = self.hash(row[1:], random_vectors)\n",
    "                hash_table[hash_value].append(row)\n",
    "            self.tables.append((random_vectors, hash_table))\n",
    "\n",
    "    def hash(self, input_vector, random_vectors):\n",
    "        bools = np.dot(input_vector, random_vectors) > 0\n",
    "        binary_string = ''.join(str(b) for b in np.where(bools, '1', '0')[0].tolist())\n",
    "        return int(binary_string, 2)\n",
    "\n",
    "    #############################################################\n",
    "    ########     second rock star building the index     ########\n",
    "    #############################################################\n",
    "    def build_index(self):\n",
    "\n",
    "       # read all rows\n",
    "        rows = self.bfh.read_all()\n",
    "\n",
    "        self.lsh_training(rows)\n",
    "        return\n",
    "\n",
    "       \n",
    "        ###################### level 2 ######################  \n",
    "\n",
    "        centroids, cluster_labels = self.kmeans_training(rows[:, 1:], self.n_clusters1)\n",
    "        # centroids, cluster_labels = self.partial_predict(rows[:, 1:], self.n_clusters1)\n",
    "        # centroids, cluster_labels = self.lsh_training(rows[:, 1:])\n",
    "\n",
    "        clusters = [[] for _ in range(self.n_clusters1)]\n",
    "        \n",
    "        for i, label in enumerate(cluster_labels):\n",
    "            clusters[int(label)].append(rows[i])\n",
    "\n",
    "        self.write_to_file('cluster_1.bin', 'positions_cluster_1.bin', 'centroids_1.bin', clusters, centroids)\n",
    "\n",
    "\n",
    "        ###################### level 2 ######################  \n",
    "\n",
    "        centroids_level2, cluster_labels = self.kmeans_training(centroids, self.n_clusters2)\n",
    "\n",
    "        clusters = [[] for _ in range(self.n_clusters2)]\n",
    "\n",
    "        for i, label in enumerate(cluster_labels):\n",
    "            clusters[int(label)].append([i]+ list(centroids[i]))\n",
    "\n",
    "        self.write_to_file('cluster_2.bin', 'positions_cluster_2.bin', 'centroids_2.bin', clusters, centroids_level2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Result:\n",
    "    run_time: float\n",
    "    top_k: int\n",
    "    db_ids: List[int]\n",
    "    actual_ids: List[int]\n",
    "\n",
    "\n",
    "def run_queries(db, np_rows, top_k, num_runs):\n",
    "    results = []\n",
    "    for _ in range(num_runs):\n",
    "        query = np.random.random((1, 70))\n",
    "\n",
    "        tic = time.time()\n",
    "        db_ids = db.retrive(query, top_k)\n",
    "        toc = time.time()\n",
    "        run_time = toc - tic\n",
    "\n",
    "        tic = time.time()\n",
    "        actual_ids = np.argsort(np_rows.dot(query.T).T / (np.linalg.norm(\n",
    "            np_rows, axis=1) * np.linalg.norm(query)), axis=1).squeeze().tolist()[::-1]\n",
    "        toc = time.time()\n",
    "        np_run_time = toc - tic\n",
    "\n",
    "        results.append(Result(run_time, top_k, db_ids, actual_ids))\n",
    "    return results\n",
    "\n",
    "\n",
    "def eval(results: List[Result]):\n",
    "    # scores are negative. So getting 0 is the best score.\n",
    "    scores = []\n",
    "    run_time = []\n",
    "    for res in results:\n",
    "        run_time.append(res.run_time)\n",
    "        # case for retireving number not equal to top_k, socre will be the lowest\n",
    "        if len(set(res.db_ids)) != res.top_k or len(res.db_ids) != res.top_k:\n",
    "            print('not equal length')\n",
    "            scores.append(-1 * len(res.actual_ids) * res.top_k)\n",
    "            continue\n",
    "        score = 0\n",
    "        for id in res.db_ids:\n",
    "            try:\n",
    "                ind = res.actual_ids.index(id)\n",
    "                if ind > res.top_k * 3:\n",
    "                    print('not in first section')\n",
    "                    score -= ind\n",
    "            except:\n",
    "                print('id not exist')\n",
    "                score -= len(res.actual_ids)\n",
    "        scores.append(score)\n",
    "\n",
    "    return sum(scores) / len(scores), sum(run_time) / len(run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\college\\CMP Fourth Year\\Advanced Databases\\project\\startercode\\project\\Semantic-Search-Engine\\cleaned.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/college/CMP%20Fourth%20Year/Advanced%20Databases/project/startercode/project/Semantic-Search-Engine/cleaned.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# indexing and searching if saved_db created\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/college/CMP%20Fourth%20Year/Advanced%20Databases/project/startercode/project/Semantic-Search-Engine/cleaned.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m db \u001b[39m=\u001b[39m IvfDb(file_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msaved_db.bin\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/college/CMP%20Fourth%20Year/Advanced%20Databases/project/startercode/project/Semantic-Search-Engine/cleaned.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m db\u001b[39m.\u001b[39;49mbuild_index()\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/college/CMP%20Fourth%20Year/Advanced%20Databases/project/startercode/project/Semantic-Search-Engine/cleaned.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m records_np \u001b[39m=\u001b[39m db\u001b[39m.\u001b[39mrertive_embeddings()\n",
      "\u001b[1;32me:\\college\\CMP Fourth Year\\Advanced Databases\\project\\startercode\\project\\Semantic-Search-Engine\\cleaned.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/college/CMP%20Fourth%20Year/Advanced%20Databases/project/startercode/project/Semantic-Search-Engine/cleaned.ipynb#W5sZmlsZQ%3D%3D?line=265'>266</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_index\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/college/CMP%20Fourth%20Year/Advanced%20Databases/project/startercode/project/Semantic-Search-Engine/cleaned.ipynb#W5sZmlsZQ%3D%3D?line=266'>267</a>\u001b[0m \n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/college/CMP%20Fourth%20Year/Advanced%20Databases/project/startercode/project/Semantic-Search-Engine/cleaned.ipynb#W5sZmlsZQ%3D%3D?line=267'>268</a>\u001b[0m    \u001b[39m# read all rows\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/college/CMP%20Fourth%20Year/Advanced%20Databases/project/startercode/project/Semantic-Search-Engine/cleaned.ipynb#W5sZmlsZQ%3D%3D?line=268'>269</a>\u001b[0m     rows \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbfh\u001b[39m.\u001b[39mread_all()\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/college/CMP%20Fourth%20Year/Advanced%20Databases/project/startercode/project/Semantic-Search-Engine/cleaned.ipynb#W5sZmlsZQ%3D%3D?line=270'>271</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlsh_training(rows)\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/college/CMP%20Fourth%20Year/Advanced%20Databases/project/startercode/project/Semantic-Search-Engine/cleaned.ipynb#W5sZmlsZQ%3D%3D?line=271'>272</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/college/CMP%20Fourth%20Year/Advanced%20Databases/project/startercode/project/Semantic-Search-Engine/cleaned.ipynb#W5sZmlsZQ%3D%3D?line=274'>275</a>\u001b[0m     \u001b[39m###################### level 2 ######################  \u001b[39;00m\n",
      "\u001b[1;32me:\\college\\CMP Fourth Year\\Advanced Databases\\project\\startercode\\project\\Semantic-Search-Engine\\cleaned.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/college/CMP%20Fourth%20Year/Advanced%20Databases/project/startercode/project/Semantic-Search-Engine/cleaned.ipynb#W5sZmlsZQ%3D%3D?line=249'>250</a>\u001b[0m \u001b[39mfor\u001b[39;00m num_hashes, num_buckets \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_hashes, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_buckets):\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/college/CMP%20Fourth%20Year/Advanced%20Databases/project/startercode/project/Semantic-Search-Engine/cleaned.ipynb#W5sZmlsZQ%3D%3D?line=250'>251</a>\u001b[0m     random_vectors \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandn(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim, num_hashes)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/college/CMP%20Fourth%20Year/Advanced%20Databases/project/startercode/project/Semantic-Search-Engine/cleaned.ipynb#W5sZmlsZQ%3D%3D?line=251'>252</a>\u001b[0m     hash_table \u001b[39m=\u001b[39m {i: [] \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(num_buckets)}\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/college/CMP%20Fourth%20Year/Advanced%20Databases/project/startercode/project/Semantic-Search-Engine/cleaned.ipynb#W5sZmlsZQ%3D%3D?line=252'>253</a>\u001b[0m     \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m rows:\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/college/CMP%20Fourth%20Year/Advanced%20Databases/project/startercode/project/Semantic-Search-Engine/cleaned.ipynb#W5sZmlsZQ%3D%3D?line=253'>254</a>\u001b[0m         hash_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhash(row[\u001b[39m1\u001b[39m:], random_vectors)\n",
      "\u001b[1;32me:\\college\\CMP Fourth Year\\Advanced Databases\\project\\startercode\\project\\Semantic-Search-Engine\\cleaned.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/college/CMP%20Fourth%20Year/Advanced%20Databases/project/startercode/project/Semantic-Search-Engine/cleaned.ipynb#W5sZmlsZQ%3D%3D?line=249'>250</a>\u001b[0m \u001b[39mfor\u001b[39;00m num_hashes, num_buckets \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_hashes, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_buckets):\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/college/CMP%20Fourth%20Year/Advanced%20Databases/project/startercode/project/Semantic-Search-Engine/cleaned.ipynb#W5sZmlsZQ%3D%3D?line=250'>251</a>\u001b[0m     random_vectors \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandn(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim, num_hashes)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/college/CMP%20Fourth%20Year/Advanced%20Databases/project/startercode/project/Semantic-Search-Engine/cleaned.ipynb#W5sZmlsZQ%3D%3D?line=251'>252</a>\u001b[0m     hash_table \u001b[39m=\u001b[39m {i: [] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_buckets)}\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/college/CMP%20Fourth%20Year/Advanced%20Databases/project/startercode/project/Semantic-Search-Engine/cleaned.ipynb#W5sZmlsZQ%3D%3D?line=252'>253</a>\u001b[0m     \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m rows:\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/college/CMP%20Fourth%20Year/Advanced%20Databases/project/startercode/project/Semantic-Search-Engine/cleaned.ipynb#W5sZmlsZQ%3D%3D?line=253'>254</a>\u001b[0m         hash_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhash(row[\u001b[39m1\u001b[39m:], random_vectors)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# indexing and searching if saved_db created\n",
    "\n",
    "db = IvfDb(file_path=\"saved_db.bin\")\n",
    "db.build_index()\n",
    "records_np = db.rertive_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "not equal length\n",
      "not equal length\n",
      "not equal length\n",
      "not equal length\n",
      "not equal length\n",
      "not equal length\n",
      "not equal length\n",
      "not equal length\n",
      "not equal length\n",
      "not equal length\n",
      "(-5000.0, 0.00010013580322265625)\n"
     ]
    }
   ],
   "source": [
    "res = run_queries(db, records_np, 5, 10)\n",
    "print(eval(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "random_vectors = np.random.randn(70, 2)\n",
    "def hash(input_vector):\n",
    "    # Normalize the input vector and the random vectors\n",
    "    norm_input_vector = input_vector / np.linalg.norm(input_vector)\n",
    "    norm_random_vectors = random_vectors / np.linalg.norm(random_vectors, axis=0)\n",
    "\n",
    "    # Compute the cosine similarity\n",
    "    cos_sim = np.dot(norm_input_vector, norm_random_vectors)\n",
    "\n",
    "    # Determine if the cosine similarity is greater than 0\n",
    "    bools = (cos_sim > 0).astype('int')\n",
    "\n",
    "    # Convert the boolean array to a binary string and then to an integer\n",
    "    return int(''.join(bools.astype('str')), 2)\n",
    "\n",
    "print(hash(np.random.randn(70)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "lsh_training.<locals>.hash() missing 1 required positional argument: 'input_vector'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\college\\CMP Fourth Year\\Advanced Databases\\project\\startercode\\project\\Semantic-Search-Engine\\cleaned.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/college/CMP%20Fourth%20Year/Advanced%20Databases/project/startercode/project/Semantic-Search-Engine/cleaned.ipynb#X10sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m db \u001b[39m=\u001b[39m IvfDb(file_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msaved_db.bin\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/college/CMP%20Fourth%20Year/Advanced%20Databases/project/startercode/project/Semantic-Search-Engine/cleaned.ipynb#X10sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m records_np \u001b[39m=\u001b[39m db\u001b[39m.\u001b[39mrertive_embeddings()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/college/CMP%20Fourth%20Year/Advanced%20Databases/project/startercode/project/Semantic-Search-Engine/cleaned.ipynb#X10sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m h \u001b[39m=\u001b[39m lsh_training(records_np)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/college/CMP%20Fourth%20Year/Advanced%20Databases/project/startercode/project/Semantic-Search-Engine/cleaned.ipynb#X10sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m h2 \u001b[39m=\u001b[39m lsh_training2(records_np)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/college/CMP%20Fourth%20Year/Advanced%20Databases/project/startercode/project/Semantic-Search-Engine/cleaned.ipynb#X10sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39m# print(h)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/college/CMP%20Fourth%20Year/Advanced%20Databases/project/startercode/project/Semantic-Search-Engine/cleaned.ipynb#X10sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m###### print each hash value and number of vectors in it\u001b[39;00m\n",
      "\u001b[1;32me:\\college\\CMP Fourth Year\\Advanced Databases\\project\\startercode\\project\\Semantic-Search-Engine\\cleaned.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/college/CMP%20Fourth%20Year/Advanced%20Databases/project/startercode/project/Semantic-Search-Engine/cleaned.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m hash_table \u001b[39m=\u001b[39m {}\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/college/CMP%20Fourth%20Year/Advanced%20Databases/project/startercode/project/Semantic-Search-Engine/cleaned.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m embeds:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/college/CMP%20Fourth%20Year/Advanced%20Databases/project/startercode/project/Semantic-Search-Engine/cleaned.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     hash_value \u001b[39m=\u001b[39m \u001b[39mhash\u001b[39;49m(row)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/college/CMP%20Fourth%20Year/Advanced%20Databases/project/startercode/project/Semantic-Search-Engine/cleaned.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mif\u001b[39;00m hash_value \u001b[39min\u001b[39;00m hash_table:\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/college/CMP%20Fourth%20Year/Advanced%20Databases/project/startercode/project/Semantic-Search-Engine/cleaned.ipynb#X10sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         hash_table[hash_value]\u001b[39m.\u001b[39mappend(row)\n",
      "\u001b[1;31mTypeError\u001b[0m: lsh_training.<locals>.hash() missing 1 required positional argument: 'input_vector'"
     ]
    }
   ],
   "source": [
    "############################## testing lsh\n",
    "def lsh_training(embeds):\n",
    "\n",
    "    dim = 70\n",
    "    num_hashes = 20\n",
    "    random_vectors = np.random.randn(dim, num_hashes)\n",
    "\n",
    "    # make a hash function for the lsh algorithm of row of 70 float generated form np.random.random()\n",
    "    def hash(self, input_vector):\n",
    "        bools = (np.dot(input_vector, self.random_vectors.T) > 0).astype('int')\n",
    "        return int(''.join(bools.astype('str')), 2)\n",
    "    \n",
    "    # create a hash table\n",
    "    hash_table = {}\n",
    "    for row in embeds:\n",
    "        hash_value = hash(row)\n",
    "        if hash_value in hash_table:\n",
    "            hash_table[hash_value].append(row)\n",
    "        else:\n",
    "            hash_table[hash_value] = [row]\n",
    "\n",
    "    return hash_table\n",
    "\n",
    "def lsh_training2(embeds, num_tables=10):\n",
    "\n",
    "    dim = 70\n",
    "    num_hashes = 10\n",
    "    hash_tables = []\n",
    "\n",
    "    for _ in range(num_tables):\n",
    "        random_vectors = np.random.randn(dim, num_hashes)\n",
    "\n",
    "        def hash(input_vector):\n",
    "            bools = np.dot(input_vector, random_vectors) > 0\n",
    "            binary_string = ''.join('1' if b else '0' for b in bools)\n",
    "            return int(binary_string, 2)\n",
    "\n",
    "        hash_table = {}\n",
    "        for row in embeds:\n",
    "            hash_value = hash(row)\n",
    "            if hash_value in hash_table:\n",
    "                hash_table[hash_value].append(row)\n",
    "            else:\n",
    "                hash_table[hash_value] = [row]\n",
    "\n",
    "        hash_tables.append(hash_table)\n",
    "\n",
    "    return hash_tables\n",
    "\n",
    "# test case for the function\n",
    "db = IvfDb(file_path=\"saved_db.bin\")\n",
    "records_np = db.rertive_embeddings()\n",
    "h = lsh_training(records_np)\n",
    "h2 = lsh_training2(records_np)\n",
    "# print(h)\n",
    "###### print each hash value and number of vectors in it\n",
    "for key, value in h.items():\n",
    "    print(key, len(value))\n",
    "print('---------------------------')\n",
    "# print the second level\n",
    "for key, value in h2[0].items():\n",
    "    print(key, len(value))\n",
    "\n",
    "\n",
    "def search(query_vector, hash_tables, num_results=10):\n",
    "    candidates = set()\n",
    "    for hash_table in hash_tables:\n",
    "        hash_value = hash(query_vector)\n",
    "        if hash_value in hash_table:\n",
    "            candidates.update(hash_table[hash_value])\n",
    "    distances = [(v, np.linalg.norm(query_vector - v)) for v in candidates]\n",
    "    distances.sort(key=lambda x: x[1])\n",
    "    return [v for v, d in distances[:num_results]]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
